{"cells":[{"cell_type":"markdown","id":"ef36f535-4bdc-4e2b-a22a-179372324b26","metadata":{},"source":["![walmartecomm](walmartecomm.jpg)\n","\n","Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n","\n","In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table in `PostgreSQL` database with the following features:\n","\n","# `grocery_sales`\n","- `\"index\"` - unique ID of the row\n","- `\"Store_ID\"` - the store number\n","- `\"Date\"` - the week of sales\n","- `\"Weekly_Sales\"` - sales for the given store\n","\n","Also, you have the `extra_data.parquet` file that contains complementary data:\n","\n","# `extra_data.parquet`\n","- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n","- `\"Temperature\"` - Temperature on the day of sale\n","- `\"Fuel_Price\"` - Cost of fuel in the region\n","- `\"CPI\"` â€“ Prevailing consumer price index\n","- `\"Unemployment\"` - The prevailing unemployment rate\n","- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n","- `\"Dept\"` - Department Number in each store\n","- `\"Size\"` - size of the store\n","- `\"Type\"` - type of the store (depends on `Size` column)\n","\n","You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n","- `\"Store_ID\"`\n","- `\"Month\"`\n","- `\"Dept\"`\n","- `\"IsHoliday\"`\n","- `\"Weekly_Sales\"`\n","- `\"CPI\"`\n","- \"`\"Unemployment\"`\"\n","\n","After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n","\n","|  Month | Weekly_Sales  | \n","|---|---|\n","| 1.0  |  33174.178494 |\n","|  2.0 |  34333.326579 |\n","|  ... | ...  |  \n","\n","Finally, you should save the `clean_data` and `agg_data` as the csv files.\n","\n","It is recommended to use `pandas` for this project. "]},{"cell_type":"markdown","id":"a8fa8146","metadata":{},"source":["Build a data pipeline using custom functions to extracts, transforms, aggregates, and loads e-commerce data.\n","\n","1. Create a function called extract() that combines the grocery_sales table and the extra_data.parquet file, returning a variable called merged_df containing data from both sources.\n","\n","2. Implement a function named transform() that takes the merged_df as input, fills missing numerical values (using any method of your choice), adds a column \"Month\", keeps the rows where the weekly sales are over $10,000 and drops the unnecessary columns. Ultimately, it should return a DataFrame and be stored as the clean_data variable.\n","\n","3. Define a function called avg_monthly_sales() that takes clean_data as input and returns an aggregated DataFrame containing two columns - \"Month\" and \"Avg_Sales\" (rounded to 2 decimals). You should call the function and store the results as a variable called agg_data.\n","\n","4. Create a function called load() that takes the cleaned and aggregated DataFrames, and their paths, and saves them as clean_data.csv and agg_data.csv respectively, without an index.\n","\n","5. Lastly, define a validation() function that checks whether the two csv files from the load() exist in the current working directory.\n","\n","Note: No engine is required to connect to the database, just execute your query in the SQL code cell provided, and the output will automatically be stored as a pandas DataFrame called grocery_sales, available for you to use in Python code cells."]},{"cell_type":"code","execution_count":1,"id":"59fe49dc-cda5-4d22-bb10-49e94cdb6437","metadata":{"collapsed":true,"customType":"sql","dataFrameVariableName":"grocery_sales","executionCancelledAt":null,"executionTime":4879,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedAt":1700137643962,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"SELECT * FROM grocery_sales","outputsMetadata":{"0":{"height":316,"type":"dataFrame"}},"sqlCellMode":"dataFrame","sqlSource":{"integrationId":"89e17161-a224-4a8a-846b-0adc0fe7a4b1","type":"integration"},"vscode":{"languageId":"sql"}},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1807553262.py, line 2)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    SELECT * FROM grocery_sales;\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["# -- Write your SQL query here\n","SELECT * FROM grocery_sales;"]},{"cell_type":"code","execution_count":null,"id":"c0d64ff1-a4ca-4a82-a8b4-e210244dedc1","metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","\n","# Start here...\n","def extract(db_data, extra_data):\n","    #     let's load the parquet file\n","    extra_data_df = pd.read_parquet(extra_data)\n","\n","    #     let's merge the grocery data with the extra data\n","    merged_df = db_data.merge(extra_data_df, on=\"index\")\n","\n","    return merged_df"]},{"cell_type":"code","execution_count":null,"id":"1e175cb5","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"00f95123","metadata":{},"outputs":[],"source":["# implementing the transform function\n","def transform(merged_dataframe):\n","    #     let's fill the missing values in the necessary\n","    merged_dataframe.fillna({\n","        \"Weekly_Sales\": merged_dataframe['Weekly_Sales'].mean(),\n","        \"CPI\": merged_dataframe['CPI'].mean(),\n","        \"Unemployment\": merged_dataframe['Unemployment'].mean()\n","    }, inplace=True)\n","\n","    #     let's add the new column - 'Month'\n","    merged_dataframe['Date'] = pd.to_datetime(merged_dataframe['Date'], format=\"%Y-%m-%d\")\n","    merged_dataframe['Month'] = merged_dataframe['Month'].dt.month\n","\n","    # let's filter the rows with weekly sales above $10,000\n","    merged_dataframe = merged_dataframe.loc[merged_dataframe['Weekly_Sales'] > 10000, :]\n","\n","    # let's drop all the unnecessary columns\n","    merged_dataframe = merged_dataframe.drop(\n","        [\n","            \"index\",\n","            \"Date\",\n","            \"Temperature\",\n","            \"Fuel_Price\",\n","            \"Markdown1\",\n","            \"Markdown2\",\n","            \"Markdown3\",\n","            \"Markdown4\",\n","            'Size',\n","            'Type'\n","        ],\n","        axis=1\n","    )\n","    \n","    return merged_dataframe"]},{"cell_type":"code","execution_count":null,"id":"ec60c6e0","metadata":{},"outputs":[],"source":["# let's implement the average monthly sales\n","def avg_monthly_sales(clean_data):\n","    # getting the two columns: Month & Weekly_Sales\n","    # sales = clean_data[['Month', 'Weekly_Sales']]\n","\n","    # \n","    agg_data = (\n","        clean_data.groupby(\"Month\")[\"Sales\"]\n","        .mean()\n","        .round(2)\n","        .reset_index(name=\"Avg_Sales\")\n","    )\n","    \n","    return agg_data"]},{"cell_type":"code","execution_count":null,"id":"b6d2a190","metadata":{},"outputs":[],"source":["# let's implement the load function\n","def load(full_data, full_data_path, aggregated_data, aggregated_data_path):\n","    # let's convert the dataframes to csv files\n","    full_data.to_csv(full_data_path, index=False)\n","    aggregated_data.to_csv(aggregated_data_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"a47be740","metadata":{},"outputs":[],"source":["def validation():\n","    \"\"\"Check whether the two CSV files from the load() exist in the current working directory.\"\"\"\n","    # List of CSV files to check\n","    files_to_check = [\"raw_tax_data.csv\", \"clean_tax_data.parquet\"]\n","\n","    # Check if each file exists in the current working directory\n","    for file in files_to_check:\n","        if not os.path.exists(file):\n","            print(\n","                f\"Error: File '{file}' does not exist in the current working directory.\"\n","            )\n","            return False\n","\n","    print(\"Validation successful: Both files exist in the current working directory.\")\n","    return True"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
